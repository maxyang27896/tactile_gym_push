{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, shutil\n",
    "import torch\n",
    "import omegaconf\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "\n",
    "import tactile_gym.rl_envs\n",
    "from tactile_gym.sb3_helpers.params import import_parameters\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a display to render image\n",
    "from pyvirtualdisplay import Display\n",
    "_display = Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/training_model\n"
     ]
    }
   ],
   "source": [
    "# Define model working directorys\n",
    "model_filename = 'training_model'\n",
    "work_dir = os.path.join(os.getcwd(), model_filename)\n",
    "model_dir = os.path.join(work_dir, 'model_trial_{}'.format(60))\n",
    "# work_dir = r\"/home/qt21590/Documents/Projects/tactile_gym_mbrl/training_model/random_goal_update_orn_john_guide_off\"\n",
    "print(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Mar  8 2021 17:26:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "Loaded EGL 1.5 after reload.\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 495.29.05\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 495.29.05\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n"
     ]
    }
   ],
   "source": [
    "# Load the environment \n",
    "env_name = 'object_push-v0'\n",
    "env_kwargs_file = 'env_kwargs'\n",
    "env_kwargs_dir = os.path.join(work_dir, env_kwargs_file)\n",
    "env_kwargs = omegaconf.OmegaConf.load(env_kwargs_dir)\n",
    "env_kwargs['env_modes']['additional_reward_settings'] = 'default_penalise_act'\n",
    "\n",
    "env = gym.make(env_name, **env_kwargs)\n",
    "seed = 0\n",
    "env.seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbrl.util.plot_and_save_push_data import plot_and_save_push_plots\n",
    "# reed csv\n",
    "evaluation_result_directory = os.path.join(work_dir, \"evaluation_result\")\n",
    "data_columns =  ['trial','trial_steps', 'time_steps', 'tcp_x','tcp_y','tcp_z','contact_x', 'contact_y', 'contact_z', 'goal_x', 'goal_y', 'goal_z', 'rewards', 'contact', 'dones']\n",
    "# data_columns =  ['trial','trial_steps', 'time_steps', 'tcp_x','tcp_y','tcp_z','contact_x', 'contact_y', 'contact_z', 'tcp_Rz', 'contact_Rz', 'goal_x', 'goal_y', 'goal_z', 'rewards', 'contact', 'dones']\n",
    "df = pd.read_csv(os.path.join(work_dir, 'evaluation_results.csv'), names = data_columns)\n",
    "data = df.to_numpy()\n",
    "# plot_and_save_push_plots(env, data, data_columns, 12, evaluation_result_directory, \"evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns =  ['trial','trial_steps', 'time_steps', 'tcp_x','tcp_y','tcp_z','contact_x', 'contact_y', 'contact_z', 'goal_x', 'goal_y', 'goal_z', 'rewards', 'contact', 'dones']\n",
    "# data_columns =  ['trial','trial_steps', 'time_steps', 'tcp_x','tcp_y','tcp_z','contact_x', 'contact_y', 'contact_z', 'tcp_Rz', 'contact_Rz', 'goal_x', 'goal_y', 'goal_z', 'rewards', 'contact', 'dones']\n",
    "df = pd.read_csv(os.path.join(work_dir, 'training_results.csv'), names = data_columns)\n",
    "data = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002239702829673331\n"
     ]
    }
   ],
   "source": [
    "loss_contact = False\n",
    "print(len(df.query(\"contact==@loss_contact\"))/ len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000000 -0.180000]\n",
      " [0.000000 0.180000]\n",
      " [0.160000 -0.180000]\n",
      " [0.160000 0.180000]\n",
      " [0.320000 -0.180000]\n",
      " [0.320000 0.000000]\n",
      " [0.320000 0.180000]\n",
      " [0.168129 -0.180000]\n",
      " [0.006371 -0.180000]\n",
      " [0.020365 -0.180000]]\n"
     ]
    }
   ],
   "source": [
    "def make_evaluation_goals(num_trials):\n",
    "\n",
    "    # Create evenly distributed goals along the edge\n",
    "    n_point_per_side, n_random = divmod(num_trials, len(env.goal_edges))\n",
    "    evaluation_goals = np.array([])\n",
    "    for edge in env.goal_edges:\n",
    "        # random x-axis\n",
    "        goal_edges = np.zeros((n_point_per_side, 2))\n",
    "        if edge[0] == 0:\n",
    "            if edge[1] == -1:\n",
    "                y = env.goal_y_min\n",
    "            else:\n",
    "                y = env.goal_y_max\n",
    "            x = np.linspace(env.goal_x_min, env.goal_x_max, num=n_point_per_side)\n",
    "        # random y axis\n",
    "        else:\n",
    "            if edge[0] == -1:\n",
    "                x = env.goal_x_min\n",
    "            else:\n",
    "                x = env.goal_x_max\n",
    "            y = np.linspace(env.goal_y_min, env.goal_y_max, num=n_point_per_side)\n",
    "        goal_edges[:, 0] = x\n",
    "        goal_edges[:, 1] = y\n",
    "        evaluation_goals = np.hstack([\n",
    "            *evaluation_goals,\n",
    "            *goal_edges\n",
    "        ])\n",
    "\n",
    "    # get unique goals\n",
    "    evaluation_goals = evaluation_goals.reshape(n_point_per_side*len(env.goal_edges), 2)\n",
    "    evaluation_goals = np.unique(evaluation_goals,axis=0)\n",
    "\n",
    "    # Fill in rest with random goals\n",
    "    for i in range(num_trials - len(evaluation_goals)):\n",
    "        evaluation_goals = np.append(evaluation_goals, [np.array(env.random_single_goal())], axis=0)\n",
    "    \n",
    "    return evaluation_goals\n",
    "\n",
    "print(make_evaluation_goals(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qt21590/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/hydra/utils.py:32: UserWarning: `OmegaConf.is_none()` is deprecated, see https://github.com/omry/omegaconf/issues/547\n",
      "  if OmegaConf.is_none(config):\n"
     ]
    }
   ],
   "source": [
    "# Get cfg and agent cfg\n",
    "config_file = 'cfg_dict'\n",
    "config_dir = os.path.join(work_dir, config_file)\n",
    "cfg = omegaconf.OmegaConf.load(config_dir)\n",
    "trial_length= cfg.overrides.trial_length\n",
    "\n",
    "agent_config_file = 'agent_cfg'\n",
    "agent_config_dir = os.path.join(work_dir, agent_config_file)\n",
    "agent_cfg = omegaconf.OmegaConf.load(agent_config_dir)\n",
    "\n",
    "# Re-map device\n",
    "map_location = None\n",
    "if cfg['dynamics_model']['device'] != device:\n",
    "    cfg['dynamics_model']['device'] = device\n",
    "    agent_cfg['optimizer_cfg']['device'] = device\n",
    "    map_location = torch.device(device)\n",
    "    \n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape, model_dir)\n",
    "model_env = models.ModelEnvPushing(env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator)\n",
    "# replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng, load_dir=model_dir)\n",
    "\n",
    "# Create dyanmics, replay buffer and agent \n",
    "# dynamics_model.load(work_dir, map_location=map_location)\n",
    "# replay_buffer.load(work_dir)\n",
    "# dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats  \n",
    "\n",
    "# Create agent \n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states_from_obs(obs):\n",
    "    if \"goal_excluded\" in env.observation_mode:\n",
    "        if env.planar_states == True:\n",
    "            # tcp_pos_workframe = np.zeros(3)\n",
    "            # tcp_orn_workframe = np.zeros(4)\n",
    "            cur_obj_pos_workframe = np.zeros(3)\n",
    "            # cur_obj_orn_workframe = np.zeros(4)\n",
    "\n",
    "            # tcp_pos_workframe[0:2] = obs[0:2]\n",
    "            # tcp_orn_workframe[2:4] = obs[2:4]\n",
    "            cur_obj_pos_workframe[0:2]= obs[4:6]\n",
    "            # cur_obj_orn_workframe[:, 2:4] = obs[4:6]\n",
    "        else:   \n",
    "            # tcp_pos_workframe = obs[0:3]\n",
    "            # tcp_orn_workframe = obs[0:4]\n",
    "            cur_obj_pos_workframe = obs[4:7]\n",
    "            # cur_obj_orn_workframe = obs[7:11]\n",
    "\n",
    "    else:\n",
    "        if env.planar_states == True: \n",
    "            # tcp_pos_to_goal_workframe = np.zeros(3)\n",
    "            # tcp_orn_to_goal_workframe = np.zeros(4)\n",
    "            cur_obj_pos_to_goal_workframe = np.zeros(3)\n",
    "            # cur_obj_orn_to_goal_workframe = np.zeros(4)\n",
    "\n",
    "            # tcp_pos_to_goal_workframe[0:2] = obs[0:2]\n",
    "            # tcp_orn_to_goal_workframe[2:4] = obs[0:2]\n",
    "            cur_obj_pos_to_goal_workframe[0:2]= obs[2:4]\n",
    "            # cur_obj_orn_to_goal_workframe[2:4] = obs[4:6]\n",
    "        else:\n",
    "            # tcp_pos_to_goal_workframe = obs[0:3]\n",
    "            # tcp_orn_to_goal_workframe = obs[0:4]\n",
    "            cur_obj_pos_to_goal_workframe = obs[4:7]\n",
    "            # cur_obj_orn_to_goal_workframe = obs[7:11]\n",
    "\n",
    "        # tcp_pos_workframe = obs[0:3] + env.goal_pos_workframe\n",
    "        cur_obj_pos_workframe = cur_obj_pos_to_goal_workframe + env.goal_pos_workframe\n",
    "\n",
    "    return cur_obj_pos_workframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# --- Doing env step using the agent and adding to model dataset ---\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     start_plan_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mact(obs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{})\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     next_obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdaffyduck/home/qt21590/Documents/Projects/tactile_gym_mbrl/mbrl-lib/notebooks/object_pushing_evaluation.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     plan_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_plan_time\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:684\u001b[0m, in \u001b[0;36mTrajectoryOptimizerAgent.act\u001b[0;34m(self, obs, optimizer_callback, **_kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrajectory_eval_fn(obs, action_sequences)\n\u001b[1;32m    683\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 684\u001b[0m plan \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    685\u001b[0m     trajectory_eval_fn, callback\u001b[39m=\u001b[39;49moptimizer_callback\n\u001b[1;32m    686\u001b[0m )\n\u001b[1;32m    687\u001b[0m plan_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    689\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions_to_use\u001b[39m.\u001b[39mextend([a \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m plan[: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplan_freq]])\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:558\u001b[0m, in \u001b[0;36mTrajectoryOptimizer.optimize\u001b[0;34m(self, trajectory_eval_fn, callback)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    541\u001b[0m     trajectory_eval_fn: Callable[[torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m    542\u001b[0m     callback: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    544\u001b[0m     \u001b[39m\"\"\"Runs the trajectory optimization.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \n\u001b[1;32m    546\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[39m        (tuple of np.ndarray and float): the best action sequence.\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m     best_solution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    559\u001b[0m         trajectory_eval_fn,\n\u001b[1;32m    560\u001b[0m         x0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprevious_solution,\n\u001b[1;32m    561\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_last_solution:\n\u001b[1;32m    564\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprevious_solution \u001b[39m=\u001b[39m best_solution\u001b[39m.\u001b[39mroll(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplan_freq, dims\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:172\u001b[0m, in \u001b[0;36mCEMOptimizer.optimize\u001b[0;34m(self, obj_fun, x0, callback, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_iterations):\n\u001b[1;32m    171\u001b[0m     population \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_population(mu, dispersion, population)\n\u001b[0;32m--> 172\u001b[0m     values \u001b[39m=\u001b[39m obj_fun(population)\n\u001b[1;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         callback(population, values, i)\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:681\u001b[0m, in \u001b[0;36mTrajectoryOptimizerAgent.act.<locals>.trajectory_eval_fn\u001b[0;34m(action_sequences)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrajectory_eval_fn\u001b[39m(action_sequences):\n\u001b[0;32m--> 681\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrajectory_eval_fn(obs, action_sequences)\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/planning/trajectory_opt.py:744\u001b[0m, in \u001b[0;36mcreate_trajectory_optim_agent_for_model.<locals>.trajectory_eval_fn\u001b[0;34m(initial_state, action_sequences)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrajectory_eval_fn\u001b[39m(initial_state, action_sequences):\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m model_env\u001b[39m.\u001b[39;49mevaluate_action_sequences(\n\u001b[1;32m    745\u001b[0m         action_sequences, initial_state\u001b[39m=\u001b[39;49minitial_state, num_particles\u001b[39m=\u001b[39;49mnum_particles\n\u001b[1;32m    746\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/models/model_env_pushing.py:700\u001b[0m, in \u001b[0;36mModelEnvPushing.evaluate_action_sequences\u001b[0;34m(self, action_sequences, initial_state, num_particles)\u001b[0m\n\u001b[1;32m    696\u001b[0m action_for_step \u001b[39m=\u001b[39m action_sequences[:, time_step, :]\n\u001b[1;32m    697\u001b[0m action_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrepeat_interleave(\n\u001b[1;32m    698\u001b[0m     action_for_step, num_particles, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    699\u001b[0m )\n\u001b[0;32m--> 700\u001b[0m _, rewards, dones, model_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m    701\u001b[0m     action_batch, model_state, sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    702\u001b[0m )\n\u001b[1;32m    703\u001b[0m rewards[terminated] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    704\u001b[0m terminated \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m dones\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/models/model_env_pushing.py:150\u001b[0m, in \u001b[0;36mModelEnvPushing.step\u001b[0;34m(self, actions, model_state, sample)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_goal_orn(next_observs)\n\u001b[1;32m    145\u001b[0m rewards \u001b[39m=\u001b[39m (\n\u001b[1;32m    146\u001b[0m     pred_rewards\n\u001b[1;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_fn(actions, next_observs)\n\u001b[1;32m    149\u001b[0m )\n\u001b[0;32m--> 150\u001b[0m dones \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtermination_fn(actions, next_observs, rewards)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m pred_terminals \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModelEnv doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yet support simulating terminal indicators.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/tactile_gym_mbrl/mbrl-lib/mbrl/models/model_env_pushing.py:320\u001b[0m, in \u001b[0;36mModelEnvPushing.termination\u001b[0;34m(self, act, next_obs, rewards)\u001b[0m\n\u001b[1;32m    318\u001b[0m tcp_obj_pos_workframe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(next_obs), \u001b[39m3\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    319\u001b[0m tcp_obj_orn_workframe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(next_obs), \u001b[39m4\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 320\u001b[0m cur_obj_pos_workframe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(next_obs), \u001b[39m3\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    321\u001b[0m cur_obj_orn_workframe \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(next_obs), \u001b[39m4\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    323\u001b[0m tcp_obj_pos_workframe[:, \u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m next_obs[:, \u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main PETS loop\n",
    "num_test_trials = 10\n",
    "all_rewards = []\n",
    "evaluation_result = []\n",
    "goal_reached = []\n",
    "plan_time = 0.0\n",
    "train_time = 0.0\n",
    "save_vid = True\n",
    "render = True\n",
    "\n",
    "if save_vid:\n",
    "    record_every_n_frames = 1\n",
    "    render_img = env.render(mode=\"rgb_array\")\n",
    "    render_img_size = (render_img.shape[1], render_img.shape[0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(\n",
    "        os.path.join(work_dir, \"evaluated_policy.mp4\"),\n",
    "        fourcc,\n",
    "        24.0,\n",
    "        render_img_size,\n",
    "    )\n",
    "\n",
    "for trial in range(num_test_trials):\n",
    "    obs = env.reset()    \n",
    "    agent.reset()\n",
    "    \n",
    "    done = False\n",
    "    trial_reward = 0.0\n",
    "    trial_pb_steps = 0.0\n",
    "    steps_trial = 0\n",
    "    \n",
    "    tcp_pos_workframe, _, _, _, _ = env.robot.arm.get_current_TCP_pos_vel_workframe()\n",
    "    cur_obj_pos_workframe = get_states_from_obs(obs)\n",
    "    evaluation_result.append(np.hstack([trial, \n",
    "                                    steps_trial, \n",
    "                                    trial_pb_steps,\n",
    "                                    tcp_pos_workframe, \n",
    "                                    cur_obj_pos_workframe, \n",
    "                                    env.goal_pos_workframe, \n",
    "                                    trial_reward, \n",
    "                                    False,\n",
    "                                    done]))\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        start_plan_time = time.time()\n",
    "        action = agent.act(obs, **{})\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        plan_time = time.time() - start_plan_time\n",
    "\n",
    "        if render:\n",
    "            render_img = env.render(mode=\"rgb_array\")\n",
    "        else:\n",
    "            render_img = None\n",
    "        \n",
    "        obs = next_obs\n",
    "        trial_reward += reward\n",
    "        trial_pb_steps += info[\"num_of_pb_steps\"]\n",
    "        steps_trial += 1\n",
    "        \n",
    "        tcp_pos_workframe, _, _, _, _ = env.robot.arm.get_current_TCP_pos_vel_workframe()\n",
    "        cur_obj_pos_workframe = get_states_from_obs(obs)\n",
    "        evaluation_result.append(np.hstack([trial,\n",
    "                                        steps_trial,\n",
    "                                        trial_pb_steps * env._sim_time_step,\n",
    "                                        tcp_pos_workframe, \n",
    "                                        cur_obj_pos_workframe, \n",
    "                                        env.goal_pos_workframe, \n",
    "                                        trial_reward, \n",
    "                                        info[\"tip_in_contact\"],\n",
    "                                        done]))\n",
    "\n",
    "         # use record_every_n_frames to reduce size sometimes\n",
    "        if save_vid and steps_trial % record_every_n_frames == 0:\n",
    "\n",
    "            # warning to enable rendering\n",
    "            if render_img is None:\n",
    "                sys.exit('Must be rendering to save video')\n",
    "\n",
    "            render_img = cv2.cvtColor(render_img, cv2.COLOR_BGR2RGB)\n",
    "            out.write(render_img)\n",
    "\n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "    \n",
    "    print(\"Terminated at step {} with reward {}, goal reached: {}\".format(steps_trial, trial_reward, env.single_goal_reached))\n",
    "    all_rewards.append(trial_reward)\n",
    "\n",
    "    # save goal reached data during training\n",
    "    if env.single_goal_reached:\n",
    "        goal_reached.append(trial_reward)\n",
    "    else:\n",
    "        goal_reached.append(0)\n",
    "\n",
    "if save_vid:\n",
    "    out.release()\n",
    "\n",
    "print(\"The average reward over {} episodes is {}\".format(num_test_trials, np.mean(all_rewards)))\n",
    "\n",
    "# Save data \n",
    "evaluation_result = np.array(evaluation_result)\n",
    "data_columns = ['trial','trial_steps', 'time_steps', 'tcp_x','tcp_y','tcp_z','contact_x', 'contact_y', 'contact_z', 'goal_x', 'goal_y', 'goal_z', 'rewards', 'contact', 'dones']\n",
    "df = pd.DataFrame(evaluation_result, columns = data_columns)\n",
    "pd.DataFrame(evaluation_result).to_csv(os.path.join(work_dir, \"evaluation_results.csv\"))\n",
    "\n",
    "# plot evaluation results\n",
    "evaluation_result_directory = os.path.join(work_dir, \"evaluation_result\")\n",
    "if not os.path.exists(evaluation_result_directory):\n",
    "    os.mkdir(evaluation_result_directory)\n",
    "else:\n",
    "    for filename in os.listdir(evaluation_result_directory):\n",
    "        file_path = os.path.join(evaluation_result_directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "def plot_and_save_push_plots(df, trials, directory):\n",
    "    loss_contact = False\n",
    "    for trial in range(trials):\n",
    "        fig_xy, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.plot(df.query(\"trial==@trial\")[\"tcp_x\"], df.query(\"trial==@trial\")[\"tcp_y\"], \"bs\", label='tcp psosition')\n",
    "        ax.plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_x\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_y\"], \"g+\", markersize=20)\n",
    "        ax.plot(df.query(\"trial==@trial\")[\"contact_x\"], df.query(\"trial==@trial\")[\"contact_y\"], \"rs\", label='contact psosition')\n",
    "        ax.plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_x\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_y\"], \"gx\", markersize=20)\n",
    "        ax.plot(df.query(\"trial==@trial\")[\"goal_x\"].iloc[0], df.query(\"trial==@trial\")[\"goal_y\"].iloc[0], \"x\", markersize=20, markeredgecolor=\"black\", label=\"goal position\")\n",
    "        ax.set_xlabel(\"x workframe\")\n",
    "        ax.set_ylabel(\"y workframe\")\n",
    "        ax.set_xlim([env.robot.arm.TCP_lims[0, 0], env.robot.arm.TCP_lims[0, 1]])\n",
    "        ax.set_ylim([env.robot.arm.TCP_lims[1, 0], env.robot.arm.TCP_lims[1, 1]])\n",
    "        ax.legend()\n",
    "        fig_xy.savefig(os.path.join(directory, \"workframe_plot_trial_{}.png\".format(trial)))\n",
    "        plt.close(fig_xy)\n",
    "\n",
    "        fig_time_xy, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "        axs[0].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"tcp_x\"], \"bs\", label='tcp ')\n",
    "        axs[0].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_x\"], \"g+\", markersize=20)\n",
    "        axs[0].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"contact_x\"], \"rs\", label='contact')\n",
    "        axs[0].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_x\"], \"gx\", markersize=20)\n",
    "        axs[0].set_xlabel(\"Time steps (s)\")\n",
    "        axs[0].set_ylabel(\"x axis workframe\")\n",
    "        axs[0].set_ylim([env.robot.arm.TCP_lims[0, 0], env.robot.arm.TCP_lims[0, 1]])\n",
    "        axs[0].legend()\n",
    "        axs[1].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"tcp_y\"], \"bs\", label='tcp')\n",
    "        axs[1].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"tcp_y\"], \"g+\", markersize=20)\n",
    "        axs[1].plot(df.query(\"trial==@trial\")[\"time_steps\"], df.query(\"trial==@trial\")[\"contact_y\"], \"rs\", label='contact')\n",
    "        axs[1].plot(df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"time_steps\"], df.query(\"trial==@trial\").query(\"contact==@loss_contact\")[\"contact_y\"], \"gx\", markersize=20)\n",
    "        axs[1].set_xlabel(\"Time steps (s)\")\n",
    "        axs[1].set_ylabel(\"y axis workframe\")\n",
    "        axs[1].set_ylim([env.robot.arm.TCP_lims[1, 0], env.robot.arm.TCP_lims[1, 1]])\n",
    "        axs[1].legend()\n",
    "        fig_time_xy.savefig(os.path.join(directory, \"time_plot_trial_{}.png\".format(trial)))\n",
    "        plt.close(fig_time_xy)\n",
    "\n",
    "plot_and_save_push_plots(df, num_test_trials, evaluation_result_directory)\n",
    "\n",
    "# Plot evaluation results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(all_rewards, 'bs-', goal_reached, 'rs')\n",
    "ax.set_xlabel(\"Trial\")\n",
    "ax.set_ylabel(\"Trial reward\")\n",
    "fig.savefig(os.path.join(work_dir, \"evaluation_output.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial</th>\n",
       "      <th>trial_steps</th>\n",
       "      <th>time_steps</th>\n",
       "      <th>tcp_x</th>\n",
       "      <th>tcp_y</th>\n",
       "      <th>tcp_z</th>\n",
       "      <th>contact_x</th>\n",
       "      <th>contact_y</th>\n",
       "      <th>contact_z</th>\n",
       "      <th>goal_x</th>\n",
       "      <th>goal_y</th>\n",
       "      <th>goal_z</th>\n",
       "      <th>rewards</th>\n",
       "      <th>contact</th>\n",
       "      <th>dones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>-0.002932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.081320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131284</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.081728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.046337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>-0.049114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>6.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>23.508333</td>\n",
       "      <td>0.253496</td>\n",
       "      <td>7.084626e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263106</td>\n",
       "      <td>0.056595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-57.931954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220220</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193065</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>8.940697e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.056804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      trial  trial_steps  time_steps     tcp_x         tcp_y  tcp_z  \\\n",
       "0       0.0          0.0    0.000000  0.000867  8.940697e-07    0.0   \n",
       "371     1.0          0.0    0.000000  0.000867  8.940697e-07    0.0   \n",
       "723     2.0          0.0    0.000000  0.000867  8.940697e-07    0.0   \n",
       "1080    3.0          0.0    0.000000  0.000867  8.940697e-07    0.0   \n",
       "1429    4.0          0.0    0.000000  0.000867  8.940697e-07    0.0   \n",
       "1809    5.0          0.0    0.000000  0.000867  8.940697e-07    0.0   \n",
       "2174    6.0          0.0    0.000000  0.000867  8.940697e-07    0.0   \n",
       "2520    6.0        346.0   23.508333  0.253496  7.084626e-02    0.0   \n",
       "2614    7.0          0.0    0.000000  0.000867  8.940697e-07    0.0   \n",
       "3054    8.0          0.0    0.000000  0.000867  8.940697e-07    0.0   \n",
       "3449    9.0          0.0    0.000000  0.000867  8.940697e-07    0.0   \n",
       "\n",
       "      contact_x  contact_y  contact_z    goal_x    goal_y  goal_z    rewards  \\\n",
       "0      0.001000   0.000000        0.0  0.290000 -0.002932     0.0   0.000000   \n",
       "371    0.001000   0.000000        0.0  0.290000  0.081320     0.0   0.000000   \n",
       "723    0.001000   0.000000        0.0  0.131284  0.090000     0.0   0.000000   \n",
       "1080   0.001000   0.000000        0.0  0.290000  0.081728     0.0   0.000000   \n",
       "1429   0.001000   0.000000        0.0  0.290000  0.046337     0.0   0.000000   \n",
       "1809   0.001000   0.000000        0.0  0.290000 -0.049114     0.0   0.000000   \n",
       "2174   0.001000   0.000000        0.0  0.193069  0.090000     0.0   0.000000   \n",
       "2520   0.263106   0.056595        0.0  0.193069  0.090000     0.0 -57.931954   \n",
       "2614   0.001000   0.000000        0.0  0.220220  0.090000     0.0   0.000000   \n",
       "3054   0.001000   0.000000        0.0  0.193065 -0.090000     0.0   0.000000   \n",
       "3449   0.001000   0.000000        0.0  0.290000  0.056804     0.0   0.000000   \n",
       "\n",
       "      contact  dones  \n",
       "0         0.0    0.0  \n",
       "371       0.0    0.0  \n",
       "723       0.0    0.0  \n",
       "1080      0.0    0.0  \n",
       "1429      0.0    0.0  \n",
       "1809      0.0    0.0  \n",
       "2174      0.0    0.0  \n",
       "2520      0.0    0.0  \n",
       "2614      0.0    0.0  \n",
       "3054      0.0    0.0  \n",
       "3449      0.0    0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = False\n",
    "df.query(\"contact==@value\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "288c20f4f30562b85a793c8b692fd9c626a3a2ddfa32fea47b77030b2eed9a18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tactile_gym_mbrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
