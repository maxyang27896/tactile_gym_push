{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import gym\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "\n",
    "import tactile_gym.rl_envs\n",
    "from tactile_gym.sb3_helpers.params import import_parameters\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Mar  8 2021 17:26:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "Loaded EGL 1.5 after reload.\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 495.29.05\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 495.29.05\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n"
     ]
    }
   ],
   "source": [
    "# Load the environment \n",
    "algo_name = 'ppo'\n",
    "env_name = 'object_push-v0'\n",
    "rl_params, algo_params, augmentations = import_parameters(env_name, algo_name)\n",
    "rl_params[\"max_ep_len\"] = 1000    \n",
    "rl_params[\"env_modes\"][ 'observation_mode'] = 'tactile_pose_relative_data'\n",
    "rl_params[\"env_modes\"][ 'control_mode'] = 'TCP_position_control'\n",
    "rl_params[\"env_modes\"]['movement_mode'] = 'TyRz'\n",
    "rl_params[\"env_modes\"]['traj_type'] = 'point'\n",
    "rl_params[\"env_modes\"]['task'] = \"goal_pos\"\n",
    "rl_params[\"env_modes\"]['planar_states'] = True\n",
    "rl_params[\"env_modes\"]['use_contact'] = True\n",
    "rl_params[\"env_modes\"]['terminate_early']  = True\n",
    "rl_params[\"env_modes\"]['terminate_terminate_early'] = True\n",
    "\n",
    "rl_params[\"env_modes\"]['rand_init_orn'] = True\n",
    "# rl_params[\"env_modes\"]['rand_init_pos_y'] = True\n",
    "# rl_params[\"env_modes\"]['rand_obj_mass'] = True\n",
    "\n",
    "rl_params[\"env_modes\"]['additional_reward_settings'] = 'john_guide_off_normal'\n",
    "rl_params[\"env_modes\"]['terminated_early_penalty'] =  -500\n",
    "rl_params[\"env_modes\"]['reached_goal_reward'] = 100\n",
    "rl_params[\"env_modes\"]['max_no_contact_steps'] = 40\n",
    "rl_params[\"env_modes\"]['max_tcp_to_obj_orn'] = 30/180 * np.pi\n",
    "rl_params[\"env_modes\"]['importance_obj_goal_pos'] = 1.0\n",
    "rl_params[\"env_modes\"]['importance_obj_goal_orn'] = 1.0\n",
    "rl_params[\"env_modes\"]['importance_tip_obj_orn'] = 1.0\n",
    "\n",
    "rl_params[\"env_modes\"]['mpc_goal_orn_update'] = True\n",
    "rl_params[\"env_modes\"]['goal_orn_update_freq'] = 'every_step'\n",
    "\n",
    "\n",
    "# set limits and goals\n",
    "TCP_lims = np.zeros(shape=(6, 2))\n",
    "TCP_lims[0, 0], TCP_lims[0, 1] = -0.1, 0.4  # x lims\n",
    "TCP_lims[1, 0], TCP_lims[1, 1] = -0.3, 0.3  # y lims\n",
    "TCP_lims[2, 0], TCP_lims[2, 1] = -0.0, 0.0  # z lims\n",
    "TCP_lims[3, 0], TCP_lims[3, 1] = -0.0, 0.0  # roll lims\n",
    "TCP_lims[4, 0], TCP_lims[4, 1] = -0.0, 0.0  # pitch lims\n",
    "TCP_lims[5, 0], TCP_lims[5, 1] = -180 * np.pi / 180, 180 * np.pi / 180  # yaw lims\n",
    "\n",
    "# goal parameter\n",
    "goal_edges = [(0, -1), (0, 1), (1, 0)] # Top bottom and stright\n",
    "# goal_edges = [(1, 0)]\n",
    "goal_x_max = np.float64(TCP_lims[0, 1] * 0.8).item()\n",
    "goal_x_min = 0.0 # np.float64(TCP_lims[0, 0] * 0.6).item()\n",
    "goal_y_max = np.float64(TCP_lims[1, 1] * 0.6).item()\n",
    "goal_y_min = np.float64(TCP_lims[1, 0] * 0.6).item()\n",
    "goal_ranges = [goal_x_min, goal_x_max, goal_y_min, goal_y_max]\n",
    "\n",
    "rl_params[\"env_modes\"]['tcp_lims'] = TCP_lims.tolist()\n",
    "rl_params[\"env_modes\"]['goal_edges'] = goal_edges\n",
    "rl_params[\"env_modes\"]['goal_ranges'] = goal_ranges\n",
    "\n",
    "env_kwargs={\n",
    "    'show_gui':False,\n",
    "    'show_tactile':False,\n",
    "    'obs_stacked_len': 1,\n",
    "    'max_steps':rl_params[\"max_ep_len\"],\n",
    "    'image_size':rl_params[\"image_size\"],\n",
    "    'env_modes':rl_params[\"env_modes\"],\n",
    "}\n",
    "\n",
    "# training environment\n",
    "env = gym.make(env_name, **env_kwargs)\n",
    "seed = 0\n",
    "env.seed(seed)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2,)\n",
      "[[0.002749 -0.005235 -0.158004 0.987438 -0.001386 0.005535 0.153985\n",
      "  0.988073]]\n",
      "[[0.002843 -0.005203 -0.165753 0.986167 -0.000767 0.005098 0.157203\n",
      "  0.987566]]\n",
      "[[0.002956 -0.005161 -0.164881 0.986313 -0.000455 0.004742 0.159286\n",
      "  0.987233]]\n",
      "[[0.003141 -0.005099 -0.162799 0.986659 0.000168 0.005242 0.156160\n",
      "  0.987732]]\n"
     ]
    }
   ],
   "source": [
    "print(obs_shape)\n",
    "print(act_shape)\n",
    "\n",
    "env.reset()\n",
    "for i in range(4):\n",
    "    obs, _, done, _ = env.step(env.action_space.sample())\n",
    "    print(obs)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_length = 1000\n",
    "num_trials = 10\n",
    "ensemble_size = 5\n",
    "buffer_size = 10000\n",
    "target_normalised = True\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 3,\n",
    "        \"ensemble_size\": ensemble_size,\n",
    "        \"hid_size\": 200,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": False,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.LeakyReLU\",\n",
    "            \"negative_slope\": 0.01\n",
    "        }\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "        \"target_normalize\": target_normalised,\n",
    "        \"dataset_size\": buffer_size\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 32,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qt21590/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/hydra/utils.py:32: UserWarning: `OmegaConf.is_none()` is deprecated, see https://github.com/omry/omegaconf/issues/547\n",
      "  if OmegaConf.is_none(config):\n"
     ]
    }
   ],
   "source": [
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = models.ModelEnvPushing(env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneDTransitionRewardModel(\n",
      "  (model): GaussianMLP(\n",
      "    (hidden_layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): EnsembleLinearLayer(num_members=5, in_size=10, out_size=200, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): EnsembleLinearLayer(num_members=5, in_size=200, out_size=200, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): EnsembleLinearLayer(num_members=5, in_size=200, out_size=200, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (mean_and_logvar): EnsembleLinearLayer(num_members=5, in_size=200, out_size=16, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(dynamics_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples stored 1000\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n",
    "common_util.rollout_agent_trajectories(\n",
    "    env,\n",
    "    1000, # initial exploration steps\n",
    "    planning.RandomAgent(env),\n",
    "    {}, # keyword arguments to pass to agent.act()\n",
    "    replay_buffer=replay_buffer,\n",
    "    trial_length=trial_length\n",
    ")\n",
    "\n",
    "print(\"# samples stored\", replay_buffer.num_stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(5, 8)\n",
      "torch.Size([5, 8])\n",
      "torch.Size([5, 15, 2])\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "planning_horizon = 15\n",
    "\n",
    "# Get action sequence from buffer\n",
    "data = replay_buffer.get_all()\n",
    "action_sequences = data.act[0:planning_horizon,:]\n",
    "action_sequences = np.tile(action_sequences, (5,1,1)).astype(np.float32)\n",
    "action_sequences = torch.from_numpy(action_sequences)\n",
    "# print(action_sequences.shape)\n",
    "\n",
    "# Initialise state and create model input\n",
    "initial_state = data.obs[0]\n",
    "print(initial_state.shape)\n",
    "initial_obs_batch = np.tile(initial_state, (5,1)).astype(np.float32)\n",
    "print(initial_obs_batch.shape)\n",
    "model_state = model_env.reset(initial_obs_batch, return_as_np=False)\n",
    "print(model_state['obs'].shape)\n",
    "print(action_sequences.shape)\n",
    "print(action_sequences[:, 1, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.6693e-04,  8.9407e-07, -8.9774e-02], device='cuda:0')\n",
      "tensor([-0.4178,  0.0146, -0.3882], device='cuda:0')\n",
      "tensor([-0.3934,  0.1383,  0.1007], device='cuda:0')\n",
      "tensor([ 0.2892, -0.1566,  0.1400], device='cuda:0')\n",
      "tensor([ 0.1399, -0.0492, -0.2984], device='cuda:0')\n",
      "tensor([ 0.3311,  0.4704, -0.3678], device='cuda:0')\n",
      "tensor([ 0.8319,  0.0420, -0.0200], device='cuda:0')\n",
      "tensor([ 0.4669,  1.0363, -0.0445], device='cuda:0')\n",
      "tensor([ 0.7695,  1.3614, -0.2305], device='cuda:0')\n",
      "tensor([ 0.5526,  2.0521, -0.8765], device='cuda:0')\n",
      "tensor([ 0.8042,  1.9921, -1.2258], device='cuda:0')\n",
      "tensor([ 0.8107,  2.5183, -1.1476], device='cuda:0')\n",
      "tensor([ 0.7837,  2.4401, -0.8553], device='cuda:0')\n",
      "tensor([ 1.1362,  2.9114, -0.9633], device='cuda:0')\n",
      "tensor([ 1.2109,  2.0777, -0.9531], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# -------- Test model one set of action sequences from buffer to see exploding -------\n",
    "# states still occur\n",
    "\n",
    "planning_horizon = 15\n",
    "\n",
    "# Get action sequence from buffer\n",
    "data = replay_buffer.get_all()\n",
    "action_sequences = data.act[0:planning_horizon,:]\n",
    "action_sequences = np.tile(action_sequences, (5,1,1)).astype(np.float32)\n",
    "action_sequences = torch.from_numpy(action_sequences)\n",
    "# print(action_sequences.shape)\n",
    "\n",
    "# Initialise state and create model input\n",
    "initial_state = data.obs[0]\n",
    "# print(initial_state.shape)\n",
    "initial_obs_batch = np.tile(initial_state, (5,1)).astype(np.float32)\n",
    "# print(initial_obs_batch.shape)\n",
    "model_state = model_env.reset(initial_obs_batch, return_as_np=False)\n",
    "# print(model_state['propagation_indices'])\n",
    "\n",
    "batch_size = initial_obs_batch.shape[0]\n",
    "total_rewards = torch.zeros(batch_size, 1)\n",
    "terminated = torch.zeros(batch_size, 1, dtype=bool)\n",
    "model_env.reset_batch_goals(batch_size)\n",
    "\n",
    "# print(data.obs[1][0:3])\n",
    "# print(data.next_obs[1][0:3])\n",
    "# print(data.act[1])\n",
    "\n",
    "for time_step in range(planning_horizon):\n",
    "    print(torch.mean(model_state[\"obs\"], 0)[0:3])\n",
    "    # print(model_state[\"obs\"].shape)\n",
    "    # print(torch.mean(model_state[\"obs\"]))\n",
    "    action_for_step = action_sequences[:, time_step, :]\n",
    "    # print(action_for_step[0])\n",
    "\n",
    "    # Re-initialise model state from data buffer with every time step (1 step rollouts)\n",
    "    # Comment out to do planning_horizon step rollouts\n",
    "    # initial_state = data.obs[time_step]\n",
    "    # initial_obs_batch = np.tile(initial_state, (5,1)).astype(np.float32)\n",
    "    # initial_obs_batch = torch.from_numpy(initial_obs_batch)\n",
    "    # model_state.update({'obs': initial_obs_batch})\n",
    "    # action_batch = torch.repeat_interleave(\n",
    "    #     action_for_step, 20, dim=0\n",
    "    # )\n",
    "\n",
    "    action_batch = action_for_step\n",
    "    # ---------------- Use model_env.step -----------------\n",
    "    # _, rewards, dones, model_state = model_env.step(\n",
    "    #     action_batch, model_state, sample=True\n",
    "    # )\n",
    "    # rewards[terminated] = 0\n",
    "    # terminated |= dones\n",
    "    # total_rewards += rewards\n",
    "\n",
    "    # -------------- Use one_dim_tr_model sample -------------\n",
    "    with torch.no_grad():\n",
    "        next_observs, _, _, next_model_state, = model_env.dynamics_model.sample(\n",
    "            action_batch, model_state, deterministic=False, rng=model_env._rng,\n",
    "        )\n",
    "\n",
    "    # -------------- Use model.sample_1d() --------------------\n",
    "    # with torch.no_grad():\n",
    "    #     obs = model_state[\"obs\"]\n",
    "    #     model_in = model_env.dynamics_model._get_model_input(model_state[\"obs\"], action_batch)\n",
    "    #     next_observs, _ = model_env.dynamics_model.model.sample_1d(\n",
    "    #         model_in, model_state, rng=model_env._rng, deterministic=False\n",
    "    #     )\n",
    "    #     next_observs += obs\n",
    "    #     model_state[\"obs\"] = next_observs\n",
    "\n",
    "    # -------------- Use model.forward()-------------------------\n",
    "    # with torch.no_grad():\n",
    "    #     obs = model_state[\"obs\"]\n",
    "    #     model_in = model_env.dynamics_model._get_model_input(model_state[\"obs\"], action_batch)\n",
    "    #     means, logvars = model_env.dynamics_model.model.forward(\n",
    "    #         model_in, rng=model_env._rng, propagation_indices=model_state[\"propagation_indices\"]\n",
    "    #     )\n",
    "    #     variances = logvars.exp()\n",
    "    #     stds = torch.sqrt(variances)\n",
    "    #     # stds = torch.ones((5,30))\n",
    "    #     next_observs = torch.normal(means, stds, generator=model_env._rng)\n",
    "    #     # next_observs = means\n",
    "    #     # print(torch.mean(means))\n",
    "    #     # print(torch.mean(logvars))\n",
    "    #     # print(torch.mean(stds))\n",
    "    #     if dynamics_model.target_normalizer:\n",
    "    #         next_observs = dynamics_model.target_normalizer.denormalize(next_observs)\n",
    "\n",
    "    #     if dynamics_model.target_is_delta:\n",
    "    #         next_observs += obs\n",
    "    #     model_state[\"obs\"] = next_observs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactile_gym_mbrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "288c20f4f30562b85a793c8b692fd9c626a3a2ddfa32fea47b77030b2eed9a18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
