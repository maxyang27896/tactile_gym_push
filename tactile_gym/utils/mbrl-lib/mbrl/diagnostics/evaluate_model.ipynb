{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Mar  8 2021 17:26:24\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "import argparse\n",
    "import cv2\n",
    "import gym\n",
    "import imageio\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, shutil\n",
    "import torch\n",
    "import omegaconf\n",
    "import time\n",
    "from typing import Dict, Optional, Tuple, Union, cast\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import mbrl.env.cartpole_continuous as cartpole_env\n",
    "import mbrl.env.reward_fns as reward_fns\n",
    "import mbrl.env.termination_fns as termination_fns\n",
    "import mbrl.models as models\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "\n",
    "import tactile_gym.rl_envs\n",
    "from tactile_gym.sb3_helpers.params import import_parameters\n",
    "from mbrl.util.plot_and_save_push_data import plot_and_save_push_plots\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from tactile_gym.sb3_helpers.params import import_parameters\n",
    "from tactile_gym.rl_envs.nonprehensile_manipulation.object_push.object_push_env import get_states_from_obs\n",
    "\n",
    "_display = Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "Loaded EGL 1.5 after reload.\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "GL_VERSION=4.6.0 NVIDIA 495.29.05\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60 NVIDIA\n",
      "Version = 4.6.0 NVIDIA 495.29.05\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 3090/PCIe/SSE2\n",
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qt21590/anaconda3/envs/tactile_gym_mbrl/lib/python3.9/site-packages/hydra/utils.py:32: UserWarning: `OmegaConf.is_none()` is deprecated, see https://github.com/omry/omegaconf/issues/547\n",
      "  if OmegaConf.is_none(config):\n"
     ]
    }
   ],
   "source": [
    "root_dir = r\"/home/qt21590/Documents/Projects/tactile_gym_mbrl/object_push_rl/model_based_models\"\n",
    "# model_name = \"relative_ext_space_normal_cem_best\"\n",
    "model_name = \"relative_randomised\"\n",
    "work_dir = os.path.join(root_dir, model_name)\n",
    "\n",
    "env_kwargs_dir = os.path.join(work_dir, 'env_kwargs')\n",
    "env_params = omegaconf.OmegaConf.load(env_kwargs_dir)\n",
    "\n",
    "# edit eval env modes here\n",
    "goals = [[0.0, 0.18]]\n",
    "n_eval_episodes = len(goals)\n",
    "env_params['goals'] = goals\n",
    "# n_eval_episodes = 10\n",
    "env_params[\"env_modes\"]['eval_mode'] = True\n",
    "env_params[\"env_modes\"]['eval_num'] = n_eval_episodes\n",
    "\n",
    "# Common reward function for evaluation\n",
    "env_params[\"env_modes\"]['terminated_early_penalty'] = -500\n",
    "env_params[\"env_modes\"]['reached_goal_reward'] = 100\n",
    "env_params[\"env_modes\"]['importance_obj_goal_pos'] = 1.0\n",
    "env_params[\"env_modes\"]['importance_obj_goal_orn'] = 1.0\n",
    "env_params[\"env_modes\"]['importance_tip_obj_orn'] = 1.0\n",
    "\n",
    "# set limits and goals\n",
    "TCP_lims = np.zeros(shape=(6, 2))\n",
    "TCP_lims[0, 0], TCP_lims[0, 1] = -0.1, 0.45  # x lims\n",
    "TCP_lims[1, 0], TCP_lims[1, 1] = -0.3, 0.3  # y lims\n",
    "TCP_lims[2, 0], TCP_lims[2, 1] = -0.0, 0.0  # z lims\n",
    "TCP_lims[3, 0], TCP_lims[3, 1] = -0.0, 0.0  # roll lims\n",
    "TCP_lims[4, 0], TCP_lims[4, 1] = -0.0, 0.0  # pitch lims\n",
    "TCP_lims[5, 0], TCP_lims[5, 1] = -180 * np.pi / 180, 180 * np.pi / 180  # yaw lims\n",
    "env_params[\"env_modes\"]['tcp_lims'] = TCP_lims.tolist()\n",
    "\n",
    "# Create environment\n",
    "seed = 0\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "handler_env_name = \"pybulletgym___\" + \"object_push-v0\"\n",
    "handler = util.create_handler_from_str(handler_env_name)\n",
    "eval_env = handler.make_env_from_str(handler_env_name, **env_params)\n",
    "eval_env.seed(seed)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = eval_env.observation_space.shape\n",
    "act_shape = eval_env.action_space.shape\n",
    "\n",
    "# Hyperparameters\n",
    "num_particles = 20\n",
    "lookhead_steps = 25\n",
    "\n",
    "# Get cfg and agent cfg\n",
    "config_file = 'cfg_dict'\n",
    "config_dir = os.path.join(work_dir, config_file)\n",
    "cfg = omegaconf.OmegaConf.load(config_dir)\n",
    "\n",
    "agent_config_file = 'agent_cfg'\n",
    "agent_config_dir = os.path.join(work_dir, agent_config_file)\n",
    "agent_cfg = omegaconf.OmegaConf.load(agent_config_dir)\n",
    "\n",
    "# model_num = 100\n",
    "# model_path = os.path.join(saved_model_dir, \"model_trial_{}\".format(model_num))\n",
    "model_path = os.path.join(work_dir, \"best_model\")\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape, model_path)\n",
    "model_env = models.ModelEnvPushing(eval_env, dynamics_model, termination_fn=None, reward_fn=None, generator=generator)\n",
    "model = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=num_particles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-43.5977], device='cuda:0')\n",
      "Episode reward,  0.0\n",
      "Episode steps,  0\n"
     ]
    }
   ],
   "source": [
    "# obs = eval_env.reset()\n",
    "# eval_env.make_goal(env_params['goals'][0])\n",
    "# done, state = False, None\n",
    "# episode_reward = 0.0\n",
    "# episode_length = 0\n",
    "# trial_pb_steps = 0.0\n",
    "\n",
    "# # Get model prediction using a optimised plan\n",
    "# plan = model.plan(obs, **{})\n",
    "# plan = torch.from_numpy(plan[None, :]).to(device)\n",
    "# total_rewards = model_env.evaluate_action_sequences(plan, obs, 20)\n",
    "# print(total_rewards)\n",
    "\n",
    "# print('Episode reward, ', episode_reward)\n",
    "# print('Episode steps, ', episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot observations\n",
    "def plot_data(model_obses, real_obses, env_frame):\n",
    "    \n",
    "    num_plots = model_obses.shape[-1]\n",
    "    rows = 2\n",
    "    columns = int(num_plots/2)\n",
    "    # fig, axs = plt.subplots(rows, columns, figsize=(18, 7))\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    gs = GridSpec(3, 4, figure=fig)\n",
    "\n",
    "    titles = ['tcp_x', 'tcp_y', 'tcp_orn_1', 'tcp_orn_2', 'contact_x', 'contact_y', 'contact_orn_1', 'contact_orn_2']\n",
    "\n",
    "    def plot_model_obses(axs, data, avg=False):\n",
    "        num_particles = data.shape[1]\n",
    "        if avg:\n",
    "            data = np.mean(data, axis=1,)\n",
    "            axs.plot(data, 'r', alpha=0.1)\n",
    "        else:\n",
    "            for i in range(num_particles):\n",
    "                axs.plot(data[:, i], 'r', alpha=0.1)\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            axs = fig.add_subplot(gs[i, j])\n",
    "            axs.plot(real_obses[:, i*4+j], 'b')\n",
    "            plot_model_obses(axs, model_obses[:, :, i*4+j])\n",
    "\n",
    "            # Settings for the plot\n",
    "            plt.axes(axs)\n",
    "            plt.title(titles[i*4+j])\n",
    "\n",
    "    axs = fig.add_subplot(gs[-1, :])\n",
    "    axs.imshow(env_frame)\n",
    "    axs.get_xaxis().set_visible(False)\n",
    "    axs.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Convert to image\n",
    "    fig.canvas.draw()\n",
    "    img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    img  = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    # img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    plt.close(fig)    \n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def get_model_real_obses(obs, env, agent, model_env, lookahead, particles):\n",
    "\n",
    "    env_dict = {\"env\": env}\n",
    "    plan = agent.plan(obs, **{})\n",
    "    model_obses, model_rewards, actions = util.common.rollout_model_push_env(\n",
    "        model_env,\n",
    "        obs,\n",
    "        plan=plan,\n",
    "        agent=None,\n",
    "        num_samples=particles,\n",
    "    )\n",
    "    real_obses, real_rewards, _ = handler.rollout_env(\n",
    "        cast(gym.wrappers.TimeLimit, env_dict),\n",
    "        obs,\n",
    "        lookahead,\n",
    "        agent=None,\n",
    "        plan=actions,\n",
    "    )\n",
    "\n",
    "    return model_obses, model_rewards, real_obses, real_rewards, actions\n",
    "\n",
    "# lookahead = 25\n",
    "# particles = 20\n",
    "# obs = eval_env.reset()\n",
    "#   # When using MPC, rollout model trajectories to see the controller actions\n",
    "# plan = model.plan(obs, **{})\n",
    "# model_obses, model_rewards, actions = util.common.rollout_model_push_env(\n",
    "#     model_env,\n",
    "#     obs,\n",
    "#     plan=plan,\n",
    "#     agent=None,\n",
    "#     num_samples=particles,\n",
    "# )\n",
    "\n",
    "# # Then evaluate in the environment\n",
    "# env_dict = {\"env\": eval_env}\n",
    "# real_obses, real_rewards, _ = handler.rollout_env(\n",
    "#     cast(gym.wrappers.TimeLimit, env_dict),\n",
    "#     obs,\n",
    "#     lookahead,\n",
    "#     agent=None,\n",
    "#     plan=actions,\n",
    "# )\n",
    "\n",
    "# frame = eval_env.render(mode=\"rgb_array\")\n",
    "# data = (model_obses, model_rewards, real_obses, real_rewards, actions)\n",
    "# img = plot_data(data, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward,  -353.70490140928507\n",
      "Episode steps,  709\n"
     ]
    }
   ],
   "source": [
    "# Plot and output as video\n",
    "def get_predictions(obs, env, agent, model_env, lookahead, particles):\n",
    "\n",
    "    data = get_model_real_obses(obs, env, agent, model_env, lookahead, particles)\n",
    "    frame = eval_env.render(mode=\"rgb_array\")\n",
    "    return data, frame\n",
    "\n",
    "# Start evaluation\n",
    "obs = eval_env.reset()\n",
    "eval_env.make_goal(env_params['goals'][0])\n",
    "done, state = False, None\n",
    "episode_reward = 0.0\n",
    "episode_length = 0\n",
    "graph_frames = []\n",
    "model_obses_trial = []\n",
    "real_obses_trial = []\n",
    "obses_err_trial = []\n",
    "env_frame_trial= []\n",
    "record_frequency = 5\n",
    "\n",
    "while not done:\n",
    "\n",
    "    if episode_length % record_frequency == 0:\n",
    "\n",
    "        # print(\"before\", eval_env.termination(0))\n",
    "        # pos_workframe, _ = eval_env.robot.arm.worldframe_to_workframe(\n",
    "        #         eval_env.cur_obj_pos_worldframe, np.zeros(3)\n",
    "        #     )\n",
    "        # print(\"observation\", pos_workframe)\n",
    "        # print('dist', np.linalg.norm(eval_env.cur_obj_pos_worldframe - eval_env.goal_pos_worldframe))\n",
    "        # print('target_id', eval_env.targ_traj_list_id)\n",
    "\n",
    "        # Get data \n",
    "        data_step, env_frame_step = get_predictions(obs, eval_env, model, model_env, lookahead=lookhead_steps, particles=num_particles)\n",
    "        model_obses, _, real_obses, _, _ = data_step\n",
    "        model_obses_trial.append(model_obses)\n",
    "        real_obses_trial.append(real_obses)\n",
    "\n",
    "        # Record model errors\n",
    "        if model_obses.shape[0] == real_obses.shape[0]:\n",
    "            sq_err_obses = np.square(np.swapaxes(model_obses,0,1) - real_obses)\n",
    "            # print(np.max(sq_err_obses, axis=(0, 1)))\n",
    "            obses_err_trial.append(sq_err_obses)\n",
    "            env_frame_trial.append(env_frame_step)\n",
    "\n",
    "        # Append graph frame into a video array\n",
    "        plot_img = plot_data(model_obses, real_obses, env_frame_step)\n",
    "        graph_frames.append(plot_img)\n",
    "\n",
    "        eval_env.get_step_data()\n",
    "        # print(\"after\", eval_env.termination(0))\n",
    "        # pos_workframe, _ = eval_env.robot.arm.worldframe_to_workframe(\n",
    "        #         eval_env.cur_obj_pos_worldframe, np.zeros(3)\n",
    "        #     )\n",
    "        # print(\"observation\", pos_workframe)\n",
    "        # print('dist', np.linalg.norm(eval_env.cur_obj_pos_worldframe - eval_env.goal_pos_worldframe))\n",
    "        # print('target_id', eval_env.targ_traj_list_id)\n",
    "        # print(\"...\")\n",
    "\n",
    "    action = model.act(obs, **{})\n",
    "    obs, reward, done, _info = eval_env.step(action)\n",
    "\n",
    "    episode_reward += reward\n",
    "    episode_length += 1\n",
    "\n",
    "    # if episode_length > 50:\n",
    "    #     done = True\n",
    "\n",
    "result_dir = r\"/home/qt21590/Documents/Projects/tactile_gym_mbrl/object_push_rl/results\"\n",
    "video_dir = os.path.join(result_dir, 'model_analysis')\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "video_file = os.path.join(video_dir, 'model_prediction.mp4')\n",
    "imageio.mimwrite(video_file, np.stack(graph_frames), fps=10)\n",
    "\n",
    "print('Episode reward, ', episode_reward)\n",
    "print('Episode steps, ', episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tactile_gym.sb3_helpers.rl_plot_utils import plot_error_band\n",
    "\n",
    "# Plot observations\n",
    "def plot_model_error(mse_err, max_err, min_err, total_obses_trial, env_frame, steps):\n",
    "    \n",
    "    num_plots = model_obses.shape[-1]\n",
    "    rows = 2\n",
    "    columns = int(num_plots/2)\n",
    "    # fig, axs = plt.subplots(rows, columns, figsize=(18, 7))\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    gs = GridSpec(3, 4, figure=fig)\n",
    "\n",
    "    titles = ['tcp_x', 'tcp_y', 'tcp_orn_1', 'tcp_orn_2', 'contact_x', 'contact_y', 'contact_orn_1', 'contact_orn_2']\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            axs = fig.add_subplot(gs[i, j])\n",
    "            \n",
    "            plot_error_band(\n",
    "                axs,\n",
    "                np.arange(len(mse_err)),\n",
    "                mse_err[:, i*4+j],\n",
    "                max_err[:, i*4+j],\n",
    "                min_err[:, i*4+j],\n",
    "                title=titles[i],\n",
    "                colour=\"r\",\n",
    "                error_band=True,\n",
    "            )\n",
    "\n",
    "            # Settings for the plot\n",
    "            plt.axes(axs)\n",
    "            plt.title(titles[i*4+j])\n",
    "            plt.ylim([0.0, 1.0])\n",
    "\n",
    "    # Plot total mse error\n",
    "    axs = fig.add_subplot(gs[-1, 0:2])\n",
    "    axs.plot(total_obses_trial, 'b')\n",
    "    axs.set_title('total model error')\n",
    "    axs.set_xlabel('time step')\n",
    "    axs.set_xlim([0, steps])\n",
    "    \n",
    "    # Show environment frame\n",
    "    axs = fig.add_subplot(gs[-1, 2:4])\n",
    "    axs.imshow(env_frame)\n",
    "    axs.get_xaxis().set_visible(False)\n",
    "    axs.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Convert to image\n",
    "    fig.canvas.draw()\n",
    "    img = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    img  = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close(fig)    \n",
    "    \n",
    "    return img\n",
    "\n",
    "# normalise the model errors of each dimension to [0, 1]\n",
    "obses_err_trial = np.array(obses_err_trial)\n",
    "obses_err_trial = obses_err_trial[:-1] # Skip the last few data points, since error is too big here\n",
    "norm_factor = np.max(obses_err_trial, axis=(0,1,2))\n",
    "obses_err_normalised = []\n",
    "for i in range(len(norm_factor)):\n",
    "    obses_err_normalised.append(obses_err_trial[:, :, :, i] / norm_factor[i])\n",
    "obses_err_normalised = np.moveaxis(obses_err_normalised,0,-1)\n",
    "\n",
    "# Plot model error of each dimension over time \n",
    "mse_obses_trial = []\n",
    "max_obses_trial = []\n",
    "min_obses_trial = []\n",
    "total_obses_trial = []\n",
    "graph_frames = []\n",
    "for i in range(len(obses_err_normalised)):\n",
    "    mse_obses = np.mean(obses_err_normalised[i], axis=0)\n",
    "    max_obses = np.max(obses_err_normalised[i], axis=0)\n",
    "    min_obses = np.min(obses_err_normalised[i], axis=0)\n",
    "    mse_obses_trial.append(mse_obses)\n",
    "    max_obses_trial.append(max_obses)\n",
    "    min_obses_trial.append(min_obses)\n",
    "\n",
    "    total_obses = np.sum(np.mean(mse_obses, axis=-1))\n",
    "    total_obses_trial.append(total_obses)\n",
    "\n",
    "    graph_frame = plot_model_error(mse_obses, max_obses, min_obses, total_obses_trial, env_frame_trial[i], len(obses_err_normalised))\n",
    "    graph_frames.append(graph_frame)\n",
    "\n",
    "result_dir = r\"/home/qt21590/Documents/Projects/tactile_gym_mbrl/object_push_rl/results\"\n",
    "video_dir = os.path.join(result_dir, 'model_analysis')\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "video_file = os.path.join(video_dir, 'model_prediction_normalised.mp4')\n",
    "imageio.mimwrite(video_file, np.stack(graph_frames), fps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04060813198599847\n",
      "0.040608132707686755\n",
      "(0.7300000190734863, -0.15000000596046448, 0.03999999910593033)\n",
      "[0.730000 -0.150000 0.040000]\n",
      "[0.000000 0.180000 0.000000]\n",
      "[[0.0, 0.18]]\n",
      "[0.038288 0.166470]\n",
      "0.040608132707686755\n",
      "pos [0.716470 -0.111712 0.040001]\n",
      "pos [0.716470 -0.111712 0.040001]\n"
     ]
    }
   ],
   "source": [
    "# eval_env.get_step_data()\n",
    "print(np.linalg.norm(eval_env.get_observation()[4:6] - np.array(goals[0])))\n",
    "print(eval_env.xyz_obj_dist_to_goal())\n",
    "print(eval_env.goal_pos_worldframe)\n",
    "\n",
    "pos_worldframe, _ = eval_env.robot.arm.workframe_to_worldframe(\n",
    "                eval_env.goal_pos_workframe, np.zeros(3)\n",
    "            )\n",
    "print(pos_worldframe)\n",
    "print(eval_env.goal_pos_workframe)\n",
    "print(goals)\n",
    "\n",
    "\n",
    "print(eval_env.get_observation()[4:6])\n",
    "\n",
    "(\n",
    "    cur_obj_pos_worldframe,\n",
    "    _,\n",
    ") = eval_env.get_obj_pos_worldframe()\n",
    "print(np.linalg.norm(eval_env.cur_obj_pos_worldframe - eval_env.goal_pos_worldframe))\n",
    "\n",
    "print(\"pos\", eval_env.cur_obj_pos_worldframe)\n",
    "print(\"pos\", cur_obj_pos_worldframe)\n",
    "\n",
    "\n",
    "# pos_workframe, _ = eval_env.robot.arm.worldframe_to_workframe(\n",
    "#                 cur_obj_pos_worldframe, np.zeros(3)\n",
    "#             )\n",
    "# print(pos_workframe)\n",
    "# # print(cur_obj_pos_worldframe)\n",
    "# print(np.linalg.norm(pos_workframe[0:2] - np.array(goals[0])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tactile_gym_mbrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "288c20f4f30562b85a793c8b692fd9c626a3a2ddfa32fea47b77030b2eed9a18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
